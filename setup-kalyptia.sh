#!/bin/bash

echo "üöÄ Installation de Kalyptia Core AI (DeepSeek R1 + V3 + LoRA/PEFT)"

# --- Pr√©paration environnement ---
sudo apt update && sudo apt upgrade -y
sudo apt install -y git-lfs python3 python3-pip

# Installer Git LFS
git lfs install

# --- T√©l√©charger DeepSeek R1 ---
if [ ! -d "kalyptia-r1" ]; then
  echo "‚¨áÔ∏è T√©l√©chargement de DeepSeek R1..."
  git clone https://huggingface.co/deepseek-ai/deepseek-r1
  mv deepseek-r1 kalyptia-r1
fi

# --- T√©l√©charger DeepSeek V3 ---
if [ ! -d "kalyptia-v3" ]; then
  echo "‚¨áÔ∏è T√©l√©chargement de DeepSeek V3..."
  git clone https://huggingface.co/deepseek-ai/deepseek-v3
  mv deepseek-v3 kalyptia-v3
fi

# --- Installer d√©pendances IA ---
pip install --upgrade pip
pip install vllm torch transformers peft accelerate datasets

# --- Pr√©parer script LoRA/PEFT pour chaque mod√®le ---
for model_dir in kalyptia-r1 kalyptia-v3; do
  echo "‚öôÔ∏è Configuration LoRA/PEFT pour $model_dir..."
  cat << 'EOF' > $model_dir/init_lora.py
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import LoraConfig, get_peft_model

print("üöÄ Initialisation LoRA/PEFT pour le mod√®le...")

model_name = "./"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# Config LoRA (lightweight, gratuit, adapt√© √† ton serveur)
peft_config = LoraConfig(
    task_type="CAUSAL_LM",
    r=8,
    lora_alpha=16,
    lora_dropout=0.1
)

model = get_peft_model(model, peft_config)
print("‚úÖ Mod√®le pr√™t avec LoRA/PEFT (sp√©cialisable).")
EOF
done

# --- Lancer le serveur API (par d√©faut sur DeepSeek V3) ---
echo "üöÄ Lancement de l'API vLLM sur le port 8000 avec Kalyptia-V3..."
cd kalyptia-v3
python3 -m vllm.entrypoints.api_server \
  --model ./ \
  --host 0.0.0.0 \
  --port 8000
